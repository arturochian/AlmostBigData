\documentclass[a4paper,openany]{book}
%%%%%%%%%%%%%%%%%%% POLSKIE ZNAKI &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&
%\usepackage{polski}
\usepackage[T1]{fontenc}
\usepackage[latin2]{inputenc} 
\usepackage[top=1.5cm, bottom=1.5cm, left=0.95cm, right=0.95cm]{geometry}
\usepackage{fancyhdr}
\pagestyle{fancy}
\usepackage[Bjarne]{fncychap}
\usepackage{rotating}
\usepackage{pdflscape}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{hyperref} %pakiet do dodawania hiper??cz
\hypersetup{colorlinks=true,
            linkcolor=blue,
            citecolor=blue,
            urlcolor=blue}
\title{\textbf{\LARGE{Cran download logs aggregation time summary} }}
\fancyhead[RO,LE]{\bfseries \small{A.Birek, M.Kosinski, N.Ryciak, W.Ryciuk}}
\fancyhead[RE,LO]{\bfseries \small{Cran download logs summary}}





\author{A.Birek, M.Kosiñski, N.Ryciak, W.Ryciuk}

\begin{document}



<<echo=FALSE>>=
opts_chunk$set(comment="", message=FALSE,fig.width=8, fig.height=6,tidy.opts=list(keep.blank.line=TRUE, width.cutoff=90))
@
\maketitle
\thispagestyle{fancy}
\tableofcontents
\begin{figure}[h!]
\begin{center}
\includegraphics[height=6cm]{github.png}
\end{center}
\caption{Powered by ! \href{https://github.com/MarcinKosinski/AlmostBigData}{https://github.com/MarcinKosinski/AlmostBigData}}
\end{figure}
\newpage
\chapter{Downloading data}
\thispagestyle{fancy}
Syntax used for downloading, unzipping and merging data is available in section \ref{ch:ddl}. More or less downloading looked like this and took about: 1783.13 s 
<<echo=FALSE>>=
library(stringi)
@


<<eval=FALSE>>=
start <- as.Date('2012-10-01')
today <- as.Date('2014-05-10')
all_days <- seq(start, today, by = 'day')
year <- as.POSIXlt(all_days)$year + 1900
urls <- paste0('http://cran-logs.rstudio.com/', year, '/', all_days, '.csv.gz')

destdir <- "D:/bd1/AlmostBigData/cran-logs/"
n <- length(urls)
i=1
for(i in 1:n){
   destfile <- stri_paste(destdir,as.character(all_days[i]))
   download.file(urls[i],destfile)
}
@


Unzipping files syntax looked like this and took:
<<eval=FALSE>>=
lok <- "D:/bd1/AlmostBigData"
gzpath <- character(n)
i <-1
for (i in 1:n){
   gzpath[i] <- paste(lok, "/cran-logs", all_days[i], sep="")
}
install.packages("R.utils")
library(R.utils)
for (i in 1:n){
   gunzip(gzpath[i], destname=paste(gzpath[i],".csv"),remove=TRUE)
}
@

Converting CSV files with proper delimiter syntax looked like this and time spent was:
<<eval=FALSE>>=
for (i in 1:n){
write.csv2(read.csv2(paste(gzpath[i],".csv"), sep=","), paste(gzpath[i],"_new.csv"))
}
@



\chapter{SAS Path}
\thispagestyle{fancy}
Syntax used for importing, merging and summarizing data is available in chapter \ref{ch:sas}. 
\section{Importing data}
Importing \texttt{csv} files into \textbf{SAS} syntax looked like this and took: average 0.2 s for each file, that gives \textbf{2 min 40 S} (Stoper method.)
<<eval=FALSE>>=
proc import datafile='D:/bd1/AlmostBigData/cran-logs2012-10-01 _new.csv'
out=CR.cran1 dbms=csv replace;
      delimiter = ';';
      getnames=yes; 
      run;

...

proc import datafile='D:/bd1/AlmostBigData/cran-logs2014-05-09 _new.csv'
out=CR.cran586 dbms=csv replace;
      delimiter = ';';
      getnames=yes; 
      run;
@

\section{Merging files}
Merging all those files syntax looked like this and time expired was:
<<eval=FALSE>>=
data Cr.DANE;
set
CR.cran1,
CR.cran2,
....
CR.cran586;
run;
@
Time expired:
\begin{figure}[h!]
\begin{center}
\includegraphics[height=2cm]{sas.jpg}
\end{center}
\end{figure}
\section{Summary for each variable}
Summaries of each variable syntax looked like this and time expired was:
<<eval=FALSE>>=
12   proc summary data=Cr.DANE2 print;
13   class package;
14   run;

NOTE: There were 41611796 observations read from the data set CR.DANE2.
NOTE: PROCEDURE SUMMARY used (Total process time):
      real time           29.82 seconds
      cpu time            20.06 seconds


15
16   proc summary data=Cr.DANE2 print;
17   class version;
18   run;

NOTE: There were 41611796 observations read from the data set CR.DANE2.
NOTE: PROCEDURE SUMMARY used (Total process time):
      real time           31.23 seconds
      cpu time            20.18 seconds


19
20   proc summary data=Cr.DANE2 print;
21   class r_arch;
22   run;

NOTE: There were 41611796 observations read from the data set CR.DANE2.
NOTE: PROCEDURE SUMMARY used (Total process time):
      real time           30.65 seconds
      cpu time            10.12 seconds


23
24   proc summary data=Cr.DANE2 print;
25   class r_os;
26   run;

NOTE: There were 41611796 observations read from the data set CR.DANE2.
NOTE: PROCEDURE SUMMARY used (Total process time):
      real time           30.59 seconds
      cpu time            12.58 seconds


27
28   proc summary data=Cr.DANE2 print;
29   class r_version;
30   run;

NOTE: There were 41611796 observations read from the data set CR.DANE2.
NOTE: PROCEDURE SUMMARY used (Total process time):
      real time           30.56 seconds
      cpu time            10.95 seconds


31
32   proc summary data=Cr.DANE2 print;
33   class country;
34   run;

NOTE: There were 41611796 observations read from the data set CR.DANE2.
NOTE: PROCEDURE SUMMARY used (Total process time):
      real time           30.07 seconds
      cpu time            12.48 seconds


@
\section{Sorting}
Sort procedure is required that frequency table can be computed. Unfortunately it takes over 3 minutes...
<<eval=FALSE>>=
1    proc sort data=Cr.Dane out=CR.Dane2;
2    by r_os;
3    run;

NOTE: There were 41611796 observations read from the data set CR.DANE.
NOTE: The data set CR.DANE2 has 41611796 observations and 11 variables.
NOTE: PROCEDURE SORT used (Total process time):
      real time           3:49.71
      cpu time            35.39 seconds

@

\section{Frequency tables}
\subsection{r os}
Frequency tables syntax and the time expired for \texttt{r os}:
<<eval=FALSE>>=
    proc freq data=Cr.dane2;
    tables r_os;
    run;

NOTE: Writing HTML Body file: sashtml.htm
NOTE: There were 41611796 observations read from the data set CR.DANE2.
NOTE: PROCEDURE FREQ used (Total process time):
      real time           32.60 seconds
      cpu time            5.46 seconds

@
\subsection{Packages}
Frequency tables syntax and the time expired for \texttt{packages}, grouped by \texttt{r os}:
<<eval=FALSE>>=
   proc freq data=Cr.dane2 page;
   by r_os;
   tables package;
   run;

NOTE: There were 41611796 observations read from the data set CR.DANE2.
NOTE: PROCEDURE FREQ used (Total process time):
      real time           53.25 seconds
      cpu time            32.46 seconds

@






\chapter{Traditional $\mathcal{R}$ Path}
\thispagestyle{fancy}
\section{Unmerged $\mathcal{R}$ files Path}

This is processing file by file syntax. Total time of processing was 11,5 hours.  Time spent on counting number of downloads packages and counting number of unique users for each day (sum of times of inner loop processing) was 11,38 hours. It means that creating other statistics and margin time was about 12 minutes.
<<eval=FALSE>>=
# Data frame for statistics
# one row <-> one file
summary <- data.frame("date"=0,"number_of_different_packages"=0,
                      "number_of_different_id"=0,"size"=0,"processing_time"=0,
                      "windows"=0,"linux"=0,"mac"=0)
filelist.csv <- list.files(destdir,pattern="u$",full.names=TRUE)
# Creating empty list for counting downloads of each package
statistics<-list()

numberofpackages<-0
k <- 1
start<-Sys.time()
for(f in filelist.csv){
  
  starttime <- Sys.time()
  
  t <- read.table(f,h=TRUE,sep=",")
  systems <- c("m"=0,"l"=0,"d"=0)
  n <- nrow(t)
  number_of_different_packages <- 0
  number_of_different_id <- 0
  for(i in 1:n){
    pi <- as.character(t[i,7])
    if(!is.na(pi)){
      if(is.null(statistics[[pi]])){
        numberofpackages <- numberofpackages+1
        statistics[numberofpackages]<-1
        names(statistics)[numberofpackages] <- pi
        
      } else {
        statistics[pi] <- statistics[[pi]]+1
      }
    }
    if(!is.na(t[i,10])){
      if(t[i,10]>number_of_different_id){
        sys <- stri_sub(t[i,6],1,1)
        if(!is.na(sys)){
          systems[sys] <- systems[sys] + 1
          number_of_different_id <- t[i,10]
        }
      }
    }
  }
  endtime <- Sys.time()
  
  summary[k,1] <- stri_sub(basename(f),1,stri_length(basename(f))-1)
  summary[k,2] <- numberofpackages
  summary[k,3] <- number_of_different_id
  summary[k,4] <- floor(file.info(f)$size/1000)
  summary[k,5] <- endtime-starttime
  summary[k,6] <- systems[1]
  summary[k,7] <- systems[2]
  summary[k,8] <- systems[3]
  cat(k," ",summary[k,5],"\n")
  k <- k+1
}
end <- Sys.time()
@

Here are some plots presenting shaping statistics in time.

<<echo=FALSE,fig.height=5>>=
summary<-read.table("H:/Windows7/Documents/summary.txt",sep =";",h=TRUE)
czasy100_ <- summary[-(1:100),5]
czasy100_[czasy100_<6] <- czasy100_[czasy100_<6]*60
czasy <- c(summary[1:100,5],czasy100_)
#sum(czasy)/3600 # Sum of processing times: 11,38 h
require(graphics)

t <- seq(ISOdatetime(2012,10,1,0,0,0), ISOdatetime(2014,05,8,0,0,0), "days")[-(450:458)]
plot(t, summary[,2], xaxt='n',type="l",xlab="date",ylab="paskages",main="Number of packages")
axis(1, at=as.POSIXct(unique(format(t, '%Y-%m-%d')), format='%Y-%m-%d'),
     labels=unique(format(t, '%Y-%m-%d')),tick=FALSE)

plot(t, summary[,3], xaxt='n',type="l",xlab="date",ylab="users",main="Number of users")
axis(1, at=as.POSIXct(unique(format(t, '%Y-%m-%d')), format='%Y-%m-%d'),
     labels=unique(format(t, '%Y-%m-%d')),tick=FALSE)

plot(t, summary[,4], xaxt='n',type="l",xlab="date",ylab="file size",main="Size of files")
axis(1, at=as.POSIXct(unique(format(t, '%Y-%m-%d')), format='%Y-%m-%d'),
     labels=unique(format(t, '%Y-%m-%d')),tick=FALSE)
@

Speed of data processing time was 77 kB/s. We estimated this fitting linear model to the last plot. Ohter way to estimate this time is dividing sum of size of files by sum of processing times. Then we get 85 kB/s.

<<echo=FALSE,fig.height=5>>=
plot(summary[,4],czasy,xlab="siez [kB]",ylab="time [s]",main="Processing time")
@

<<eval=FALSE,echo=FALSE,results='hide'>>=
lm(czasy~summary[,4])$coef[2] # - slope coefficient. It means speed = 1/0,013 = 77 kB/s
sum(summary[,4])/sum(czasy)
@

Percentage of operating systems: windows - 68\%, linux - 15\%, mac os - 16\%
<<echo=FALSE,fig.height=4,fig.width=4,fig.align='center'>>=
#c(sum(summary[,6]),sum(summary[,7]),sum(summary[,8]))/sum(summary[,6:8])
pie(c(sum(summary[,6]),sum(summary[,7]),sum(summary[,8])),labels=names(summary)[6:8])
# windows: 68%, linux: 15%, mac os: 16%
@




\section{Merged $\mathcal{R}$ files Path}
Merging all filles with R looked like this: 
<<eval=FALSE>>=
start <- as.Date('2012-10-01')
today <- as.Date('2014-05-09')

all_days <- seq(start, today, by = 'day')

temp_dir <- "~//BigData"
filenames <- paste0(temp_dir,"/", all_days, '_newf.csv')
for (i in 1:length(filenames)) {
dane<-read.csv2(filenames[i])
dane<-dane[c(-1)]

write.table(dane,file=paste(temp_dir, "/dane.csv", sep=""),append=TRUE,dec=",",sep=';',qmethod='double',col.names=FALSE,row.names=FALSE)
}
@
It took around 25 mins.
\subsection{Names}
Getting all packages names, architectures kinds and operating system names was done in around 15 min using the following code: 
<<eval=FALSE>>=
options(stringsAsFactors = FALSE)
temp_dir<-destdir <- "~//BigData"
fname<-paste(temp_dir, "/dane.csv", sep="")

nazwy<-vector("character")

skipy<-0
nrowsy<-2000000
nazwy<-character(0)
arch<-character(0)
r_os<-character(0)

ileLinijek<-0

   while(class(
      try({d<-read.csv2(fname,skip=skipy,nrows=nrowsy)},silent=TRUE)
   )
   !="try-error"  )
   { 
       nazwy<-union(d[,7],nazwy)
         arch<-union(d[,5],arch)
         r_os<-union(d[,6],r_os)
      ileLinijek<-ileLinijek+length(d[,1])
      skipy<-skipy+nrowsy
   }
   

nazwy<-na.omit(nazwy)
arch<-na.omit(arch)
r_os<-na.omit(r_os)
n<-length(nazwy)
a<-length(arch)
r<-length(r_os)
a_r<-data.frame(matrix(0,nrow=a,ncol=r))
rownames(a_r)<-arch
names(a_r)<-r_os


pakiety<-data.frame(rep( 0 ,length.out=n),row.names=nazwy)
names(pakiety)<-"Krotnosci"
@

\subsection{Getting Data}
To get required data we used the following code: 
<<eval=FALSE>>=
skipy<-0
nrowsy<-2000000
while(class(
   try({d<-read.csv2(fname,skip=skipy,nrows=nrowsy)},silent=TRUE)
)
!="try-error"  )
{       
         for (j in 1:length(d[,1]) )
               {if (!is.na(d[j,7]))pakiety[(d[j,7]),]<-pakiety[(d[j,7]),]+1}
               
         
        skipy<-skipy+nrowsy
      }
@

<<eval=FALSE>>=
skipy<-0
nrowsy<-2000000
while(class(
   try({d<-read.csv2(fname,skip=skipy,nrows=nrowsy)},silent=TRUE)
)
!="try-error"  )
{ 
         un<-unique(d[,10])
         un<-na.omit(un)
         for (i in 1:length(un))
         {
            s<-(d[d[,10]==un[i],][1,])
            aa<-s[1,6]
            rr<-s[1,5]
            if (!is.na(aa) & !is.na(rr))a_r[rr,aa]<-a_r[rr,aa]+1
         }

             skipy<-skipy+nrowsy
      }
@

First took around 2 hours, and second 3 hours.





\chapter{Rcpp Path}
\thispagestyle{fancy}
\appendix
\chapter{Preparing report}
\section{Data download, unzipp, conversion syntax}\label{ch:ddl}
\thispagestyle{fancy}
<<eval=FALSE>>=
start <- as.Date('2012-10-01')
today <- as.Date('2014-05-10')
all_days <- seq(start, today, by = 'day')
year <- as.POSIXlt(all_days)$year + 1900
urls <- paste0('http://cran-logs.rstudio.com/', year, '/', all_days, '.csv.gz')

destdir <- "D:/bd1/AlmostBigData/cran-logs/"
n <- length(urls)
i=1
for(i in 1:n){
   destfile <- stri_paste(destdir,as.character(all_days[i]))
   download.file(urls[i],destfile)
}

lok <- "D:/bd1/AlmostBigData"
gzpath <- character(n)
i <-1
for (i in 1:n){
   gzpath[i] <- paste(lok, "/cran-logs", all_days[i], sep="")
}
install.packages("R.utils")
library(R.utils)
for (i in 1:n){
   gunzip(gzpath[i], destname=paste(gzpath[i],".csv"),remove=TRUE)
}

for (i in 1:n){
write.csv2(read.csv2(paste(gzpath[i],".csv"), sep=","), paste(gzpath[i],"_new.csv"))
}
@
\chapter{SAS Syntax}\label{ch:sas}
<<eval=FALSE>>=
proc import datafile='D:/bd1/AlmostBigData/cran-logs2012-10-01 _new.csv'
out=CR.cran1 dbms=csv replace;
      delimiter = ';';
      getnames=yes; 
      run;

...

proc import datafile='D:/bd1/AlmostBigData/cran-logs2014-05-09 _new.csv'
out=CR.cran586 dbms=csv replace;
      delimiter = ';';
      getnames=yes; 
      run;

data Cr.DANE;
set
CR.cran1,
CR.cran2,
....
CR.cran586;
run;

proc summary data=Cr.DANE print;
class package;
run;

proc summary data=Cr.DANE print;
class version;
run;

proc summary data=Cr.DANE print;
class r_arch;
run;

proc summary data=Cr.DANE print;
class r_os;
run;

proc summary data=Cr.DANE print;
class r_version;
run;

proc summary data=Cr.DANE print;
class country;
run;

proc sort data=Cr.Dane out=CR.Dane2;
by r_os;
run;

proc freq data=Cr.dane2 page;
tables r_os;
run;


proc freq data=Cr.dane2 page;
by r_os;
tables package;
run;
@

\chapter{Summary statistics for days}\label{ch:sum}

<<echo=FALSE>>=
summary<-read.table("H:/Windows7/Documents/summary.txt",sep =";",h=TRUE)
names(summary) <- c("date","packages","users", "size","processing_time","windows","linux","mac")
summary
@


\end{document}